{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natu- NLP- Week 1 - Word2Vec - Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nlauchande/corise-nlp-notebooks/blob/main/Natu_NLP_Week_1_Word2Vec_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziQY_aIHBcLR"
      },
      "source": [
        "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
        "\n",
        "\n",
        "### Word2Vec\n",
        "\n",
        "In this notebook we're going to learn and play around with word2vec embeddings that are packaged with Spacy. We'll try to build intuition on how they work and what can we do with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2T-deBX3huM"
      },
      "source": [
        "Install all the required dependencies for the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj-w6eV7_abU"
      },
      "source": [
        "%%capture\n",
        "!pip install spacy==2.2.4 --quiet\n",
        "!python -m spacy download en_core_web_md\n",
        "!apt install libopenblas-base libomp-dev\n",
        "!pip install faiss==1.5.3 --quiet"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnFmq6hM3n4D"
      },
      "source": [
        "Import all the necessary libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSDbwEoL8-0r"
      },
      "source": [
        "from collections import defaultdict\n",
        "import en_core_web_md\n",
        "import numpy as np\n",
        "import spacy\n",
        "import time\n",
        "import faiss"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQa8P1KPt4xP"
      },
      "source": [
        "Now let's load the Spacy data which comes with pre-trainined embeddings. This process is expensive so only do this once.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA_Xwhmnt2Ph"
      },
      "source": [
        "spacyModel = en_core_web_md.load()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vnj8WIrujBZ"
      },
      "source": [
        "First, let's play with some basic similarity functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnRogtIDuekU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5883722-0745-472a-b826-2e458a07f4e4"
      },
      "source": [
        "banana = spacyModel(\"banana\")\n",
        "fruit = spacyModel(\"fruit\")\n",
        "table = spacyModel(\"table\")\n",
        "print(banana.similarity(fruit))\n",
        "print(banana.similarity(table))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.671483588786149\n",
            "0.22562773991991913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp361KZ32XUR"
      },
      "source": [
        "As expected `Banana` is a lot more similar to `Fruit` than to `Table`. Now let's iterate over the entire vocabulary and build a search index using **Faiss**. This will make it a lot faster for us to find similar words instead of having to loop over the entire corpus each time. \n",
        "\n",
        "Feel free to ignore learning more about **Faiss** at this time as we'll dive more into it in Week 3. At the high-level it is a really efficient library to find similar vectors from a corpus.\n",
        "\n",
        "Note: This next cell will take a fair bit of time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNKVP0eYBbZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d734730-78a8-4936-c00b-df8b0bc8b5b0"
      },
      "source": [
        "def load_index():\n",
        "  \"\"\"Expensive method - call only once!!\n",
        "  \"\"\"\n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "  vectors = []\n",
        "  vector_dimension = 300\n",
        "  id = 0\n",
        "\n",
        "  # Iterate over the entire vocabulary\n",
        "  for i, tok in enumerate(spacyModel.vocab):\n",
        "    vector = tok.vector\n",
        "    l2_norm = np.linalg.norm(vector)\n",
        "\n",
        "    # Ignore zero vectors, nan vlaues\n",
        "    if (np.isclose(l2_norm, 0.0) or \n",
        "        np.isnan(l2_norm) or \n",
        "        np.any(np.isnan(vector))):\n",
        "      continue\n",
        "    else:\n",
        "      vectors.append(np.divide(vector, l2_norm))\n",
        "\n",
        "    # Add to the output variables\n",
        "    word_to_id[tok.text.lower()] = id\n",
        "    id_to_word[id] = tok.text.lower()\n",
        "    id += 1\n",
        "\n",
        "  \n",
        "  vectors = np.array(vectors)\n",
        "  index = faiss.IndexFlatIP(vector_dimension)\n",
        "  index.add(vectors)\n",
        "  return word_to_id, id_to_word, vectors, index\n",
        "\n",
        "word_to_id, id_to_word, vectors, index = load_index()\n",
        "vector_size = len(vectors)\n",
        "print(\"We created a search index of %d vectors\" % vector_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We created a search index of 684754 vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAQitr3R4bLA"
      },
      "source": [
        "Now we're going to add a helper functions to calculate top_k similar words to some input in the index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8rGyqh1F5O6"
      },
      "source": [
        "def search_vector(word_vector, top_k=100, print_time_taken=False):\n",
        "  word_vector = np.array([word_vector])\n",
        "  start_time = time.time()\n",
        "  scores, indices = index.search(word_vector, top_k)\n",
        "  if print_time_taken:\n",
        "    print(\"Time taken to search amongst {} words is {:.3}s\".format(\n",
        "        vector_size, (time.time() - start_time))\n",
        "    )\n",
        "  results = []\n",
        "  words = set()\n",
        "  for i, query_index in enumerate(indices):\n",
        "      # Matches for the i'th one \n",
        "      for inn_idx, word_index in enumerate(query_index):\n",
        "          if word_index < 0:\n",
        "              continue\n",
        "          word = id_to_word[word_index]\n",
        "          if word in words:\n",
        "            continue\n",
        "          words.add(word)\n",
        "          results.append((word, float(scores[i][inn_idx])))\n",
        "  return sorted(results, key=lambda tup: -tup[1])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcmoOvfA54JS"
      },
      "source": [
        "Let's do an empirical test by searching similar words to a few terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSriAnCr9g7c"
      },
      "source": [
        "### Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaaWpQ97oCre"
      },
      "source": [
        "def search(word, top_k=100,print_time_taken=False):\n",
        "  word = word.lower()\n",
        "  if word not in word_to_id:\n",
        "    print(\"Oops, the word {} is not in loaded dictionary\".format(word))\n",
        "    return\n",
        "  id = word_to_id[word]\n",
        "  word_vector = vectors[id]\n",
        "  search_results = search_vector(word_vector, top_k, print_time_taken)\n",
        "  print(f\"The top similar words to {word} are - \")\n",
        "  for i in range(len(search_results)):\n",
        "    print(f\"Word = {search_results[i][0]} and similarity = {search_results[i][1]}\")\n",
        "  return search_results"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6iSiVNv7A6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3936501b-d4ac-4ed9-f85f-03a998be2a49"
      },
      "source": [
        "output = search(\"happy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar words to happy are - \n",
            "Word = happy and similarity = 0.9999999403953552\n",
            "Word = glad and similarity = 0.7701864242553711\n",
            "Word = hope and similarity = 0.7318377494812012\n",
            "Word = everyone and similarity = 0.7277782559394836\n",
            "Word = thankful and similarity = 0.6912474632263184\n",
            "Word = excited and similarity = 0.6901208162307739\n",
            "Word = love and similarity = 0.6869317889213562\n",
            "Word = wish and similarity = 0.6853763461112976\n",
            "Word = gratefully and similarity = 0.6847965717315674\n",
            "Word = greatful and similarity = 0.6847965717315674\n",
            "Word = grateful and similarity = 0.6847965717315674\n",
            "Word = appreciative and similarity = 0.6847965717315674\n",
            "Word = always and similarity = 0.6795146465301514\n",
            "Word = lucky and similarity = 0.6788508296012878\n",
            "Word = feel and similarity = 0.6768770813941956\n",
            "Word = freinds and similarity = 0.6759893894195557\n",
            "Word = friends and similarity = 0.6759893894195557\n",
            "Word = thank and similarity = 0.6728546619415283\n",
            "Word = really and similarity = 0.6657894849777222\n",
            "Word = sure and similarity = 0.6657199859619141\n",
            "Word = too and similarity = 0.6657150983810425\n",
            "Word = so and similarity = 0.6655703783035278\n",
            "Word = adored and similarity = 0.6629008054733276\n",
            "Word = loved and similarity = 0.6629008054733276\n",
            "Word = loving and similarity = 0.6582796573638916\n",
            "Word = thrilled and similarity = 0.6547297239303589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni0bFOxi7CFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb336c3-4862-45b2-bb77-a21f981d5ac3"
      },
      "source": [
        "output = search(\"baseball\", 10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar words to baseball are - \n",
            "Word = fastpitch and similarity = 0.9999999403953552\n",
            "Word = scorebook and similarity = 0.9999999403953552\n",
            "Word = baseballs and similarity = 0.9999999403953552\n",
            "Word = baseball and similarity = 0.9999999403953552\n",
            "Word = softball and similarity = 0.9999999403953552\n",
            "Word = sandlot and similarity = 0.9999999403953552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IzaN0X67M70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32465d91-7b41-4c78-8549-edd4353d4417"
      },
      "source": [
        "output = search(\"cheese\", 25)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar words to cheese are - \n",
            "Word = mozzarella and similarity = 1.0000001192092896\n",
            "Word = fromage and similarity = 1.0000001192092896\n",
            "Word = cheeses and similarity = 1.0000001192092896\n",
            "Word = cheese and similarity = 1.0000001192092896\n",
            "Word = velveta and similarity = 0.8228569626808167\n",
            "Word = chevre and similarity = 0.8228569626808167\n",
            "Word = cheddar and similarity = 0.8228569626808167\n",
            "Word = chedder and similarity = 0.8228569626808167\n",
            "Word = emmental and similarity = 0.8228569626808167\n",
            "Word = mozza and similarity = 0.8228569626808167\n",
            "Word = roquefort and similarity = 0.8228569626808167\n",
            "Word = cheeseboard and similarity = 0.8228569626808167\n",
            "Word = mozz and similarity = 0.8228569626808167\n",
            "Word = velveeta and similarity = 0.8228569626808167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovIxI9Uutgc-"
      },
      "source": [
        "Now why don't you try out a few different words that come to mind and see where does the model perform well and where it struggles!! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVPJ2_R59i-2"
      },
      "source": [
        "### Analogies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bunCp7p9rplw"
      },
      "source": [
        "def analogy(word1, word2, word3):\n",
        "  word1 = word1.lower()\n",
        "  word2 = word2.lower()\n",
        "  word3 = word3.lower()\n",
        "  if word1 not in word_to_id or word2 not in word_to_id or word3 not in word_to_id:\n",
        "    print(\"word not present in dictionary, try something else\")\n",
        "  vector1 = vectors[word_to_id[word1]]\n",
        "  vector2 = vectors[word_to_id[word2]]\n",
        "  vector3 = vectors[word_to_id[word3]]\n",
        "  analogy_results = search_vector(np.add(np.subtract(vector1, vector2), vector3), 10)\n",
        "  print(f\"The top similar item for ({word1} - {word2} + {word3}) = {analogy_results[0][0]}\")\n",
        "  print(f\"The top similar words to ({word1} - {word2} + {word3}) are - \")\n",
        "  for i in range(len(analogy_results)):\n",
        "    print(f\"Word = {analogy_results[i][0]} and similarity = {analogy_results[i][1]}\")\n",
        "  return analogy_results"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N6uPUoQ5i15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327762e7-ea66-4138-912d-4b56c65878c5"
      },
      "source": [
        "output = analogy(\"salad\", \"patty\", \"bun\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar item for (salad - patty + bun) = topknot\n",
            "The top similar words to (salad - patty + bun) are - \n",
            "Word = topknot and similarity = 0.9501674771308899\n",
            "Word = buttie and similarity = 0.9501674771308899\n",
            "Word = bun and similarity = 0.9501674771308899\n",
            "Word = chignon and similarity = 0.9501674771308899\n",
            "Word = plait and similarity = 0.9501674771308899\n",
            "Word = nuong and similarity = 0.9501674771308899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XkbyKrc7ySI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61840a14-f3c0-4799-ecef-981ef5786c37"
      },
      "source": [
        "output = analogy(\"smallest\", \"small\", \"short\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar item for (smallest - small + short) = shortest\n",
            "The top similar words to (smallest - small + short) are - \n",
            "Word = shortest and similarity = 0.8045328259468079\n",
            "Word = straightest and similarity = 0.8045328259468079\n",
            "Word = second-longest and similarity = 0.7076637744903564\n",
            "Word = longest-ever and similarity = 0.7076637744903564\n",
            "Word = fifth-longest and similarity = 0.7076637744903564\n",
            "Word = third-longest and similarity = 0.7076637744903564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4n1n870tqVS"
      },
      "source": [
        "Now why don't you try out a few different examples see what comes out :) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = analogy(\"salad\", \"patty\", \"bun\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ5gu-cg-bCr",
        "outputId": "eca05de3-1540-4002-fd70-f0d74fb4d86f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar item for (salad - patty + bun) = topknot\n",
            "The top similar words to (salad - patty + bun) are - \n",
            "Word = topknot and similarity = 0.9501674771308899\n",
            "Word = buttie and similarity = 0.9501674771308899\n",
            "Word = bun and similarity = 0.9501674771308899\n",
            "Word = chignon and similarity = 0.9501674771308899\n",
            "Word = plait and similarity = 0.9501674771308899\n",
            "Word = nuong and similarity = 0.9501674771308899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = analogy(\"fish\", \"wrap\", \"cheese\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0DlSLAm-cEb",
        "outputId": "45d37c47-40d0-4eb8-ee59-56049d49ca9d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top similar item for (fish - wrap + cheese) = warmwater\n",
            "The top similar words to (fish - wrap + cheese) are - \n",
            "Word = warmwater and similarity = 1.1421476602554321\n",
            "Word = carp and similarity = 1.1421476602554321\n",
            "Word = catfish and similarity = 1.1421476602554321\n",
            "Word = bream and similarity = 1.1421476602554321\n",
            "Word = fishes and similarity = 1.1421476602554321\n",
            "Word = freshwater and similarity = 1.1421476602554321\n",
            "Word = fish and similarity = 1.1421476602554321\n",
            "Word = perch and similarity = 1.1421476602554321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iTG116X_-gua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP- Week 3 - FAISS - Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nlauchande/corise-nlp-notebooks/blob/main/NLP_Week_3_FAISS_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
        "\n",
        "\n",
        "# Week 3: FAISS Tutorial\n",
        "\n",
        "### What we are looking at\n",
        "The goal of this small tutorial, is to provide you a quick overview into what FAISS does and how you can utilize it for Week 3 project. FAISS is an index for efficiently storing searchable embeddings of objects (e.g. sentences, images, ...). This efficient storing allows us to quickly compare our current object against the objects present in the index, and thus find relevant similar results. FAISS uses approximate nearest neighbor search to achieve these quick results.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Go through all the steps and look at what kind of embeddings we create.\n",
        "1. Feel free to add more sentences to be embedded.\n",
        "1. Make sure to have a look at the interactive graph, and see how close some results are, and how some are not. Does it make sense?\n",
        "1. Have a look at the results retrieved from the FAISS index we made. Are they appropriate? Try and play around with the number of results it retrieves.\n",
        "\n",
        "### Code Overview\n",
        "\n",
        "- Dependencies: Install and import python dependencies\n",
        "- Dataset creation\n",
        "- Cohere API\n",
        "- Creating a FAISS index\n"
      ],
      "metadata": {
        "id": "D3Q13pqVM-8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies\n",
        "\n",
        "âœ¨ Now let's get started! To kick things off, as always, we will install some dependencies."
      ],
      "metadata": {
        "id": "faxj21eGPeQh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoH9De1v8-ez"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install cohere umap-learn faiss altair\n",
        "!apt install libopenblas-base libomp-dev"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all the necessary libraries we need throughout the project. Make sure to create a Cohere account and create an API key: https://os.cohere.ai/"
      ],
      "metadata": {
        "id": "vkBg-06IPkop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "import umap\n",
        "import faiss\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "\n",
        "COHERE_API_KEY = \"YOUR COHERE API KEY\"\n",
        "co = cohere.Client(COHERE_API_KEY)"
      ],
      "metadata": {
        "id": "wPNkMb399Ngh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset creation\n",
        "\n",
        "Below we create our own small dataset, and its WONDERFULðŸ¤©. Please feel free to add your own examples to it, the more the betterâœ¨âœ¨! We make use of Cohere to quickly retrieve sentence embeddings that can be used for storing in our FAISS index."
      ],
      "metadata": {
        "id": "vy8qFO6RPnzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "             # Movies\n",
        "             \"I am watching a movie.\",\n",
        "             \"I'm going to the movies.\",\n",
        "             \"Cinema's popcorn smell is amazing.\",\n",
        "             \"These guys kept talking while I was watching the movie.\",\n",
        "             # Groceries\n",
        "             \"Groceries are expensive now?\",\n",
        "             \"What happend to all my groceries, they are all rotten.\",\n",
        "             \"I like avocado toast\",\n",
        "             \"Cheese is over there!\",\n",
        "             \"Spinach is the food of the gods.\",\n",
        "             \"Healthy dose of protein powder is always good.\",\n",
        "             # Music\n",
        "             \"Coldplay is not my favorite band anymore.\",\n",
        "             \"I really liked MTV, with all the video clips.\",\n",
        "             \"What music would you like me to play?\",\n",
        "             \"He's playing piano very well.\"\n",
        "             ]\n",
        "\n",
        "df = pd.DataFrame (sentences, columns = ['conversation'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "24IDW29Q-K4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cohere API: U there?\n",
        "Here we retrieve the sentence embeddings through Cohere API. Be sure to check out the documentation: https://docs.cohere.ai/api-reference/"
      ],
      "metadata": {
        "id": "GIzW7VIwP-Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the embeddings of our sentences by calling\n",
        "# the API of Cohere.\n",
        "embeds = co.embed(texts = sentences,\n",
        "                       model = \"small\", \n",
        "                       truncate = \"LEFT\").embeddings\n",
        "\n",
        "embeds = np.array(embeds)\n",
        "embeds.shape"
      ],
      "metadata": {
        "id": "cL3MSRDMG-xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we make use of UMAP and altair. UMAP we use to reduce the dimensions of our embeddings (Small size is 1024 ðŸ˜µ). No other way to plot it then using dimensionality reduction. With Altair we make an interactive plot.\n",
        "\n",
        "\n",
        "Please hover over some of these points and see if you can identify a pattern."
      ],
      "metadata": {
        "id": "vWVGOqrCRbRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UMAP reduces dimensions from 1024 to 2, which we can plot\n",
        "reducer = umap.UMAP()\n",
        "umap_embeds = reducer.fit_transform(embeds)\n",
        "# Make interactive plot\n",
        "df_explore = pd.DataFrame(data={'text': df['conversation']})\n",
        "df_explore['x'] = umap_embeds[:,0]\n",
        "df_explore['y'] = umap_embeds[:,1]\n",
        "chart = alt.Chart(df_explore).mark_circle(size=60).encode(\n",
        "    x=alt.X('x', scale=alt.Scale(zero=False)),\n",
        "    y=alt.Y('y', scale=alt.Scale(zero=False)),\n",
        "    tooltip=['text']\n",
        ").properties(width=700, height=400)\n",
        "chart.interactive()"
      ],
      "metadata": {
        "id": "pg1WbkT1HvqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating FAISS: the good stuff.\n",
        "Creating FAISS is rather straightforward. \n",
        "1. Identify which index you want to use, with the dimension your embeddings have. \n",
        "1. Add all the embeddings you want to add.\n",
        "\n",
        "Since we made embeddings of sentences, we can now query this index with an example like *\"I like eating cabbage\"*. We turn this into a embedding and search for related sentences in our small index."
      ],
      "metadata": {
        "id": "IiHEqxq5R6Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our Approximate Nearest Neighbour Index (ANN)\n",
        "# https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n",
        "faiss_index = faiss.IndexFlatIP(1024)\n",
        "\n",
        "# Convert from float64 to float32 to prevent bug:\n",
        "# https://github.com/facebookresearch/faiss/issues/461\n",
        "faiss_index.add(np.float32(np.stack(embeds)))\n",
        "\n",
        "# Create an embedding for our sentence\n",
        "embed = co.embed(texts = [\"I like eating cabbage!\"], \n",
        "                 model = \"small\",\n",
        "                 truncate = \"LEFT\").embeddings\n",
        "\n",
        "# Get the results\n",
        "scores, indices = faiss_index.search(np.float32(np.array(embed)), 5)\n",
        "\n",
        "# Print the results\n",
        "for indice, score in zip(indices[0], scores[0]):\n",
        "  print(sentences[indice], \"\\t\\t\\t\\t\", score)"
      ],
      "metadata": {
        "id": "SNXsPkwZJbET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ¨ Tada âœ¨, hopefully the results match your expectations! \n",
        "\n",
        "ðŸ™Œ Good luck with the project! ðŸ™Œ"
      ],
      "metadata": {
        "id": "9QauKfJnS5lS"
      }
    }
  ]
}